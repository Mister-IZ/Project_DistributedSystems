ğŸ§­ **Documentation du Projet Distributed Systems â€“ Partie 1**

## 1. PrÃ©-requis

Avant de dÃ©marrer :

- Avoir **Docker Desktop** installÃ© sur votre machine.  
- Avoir **Kubernetes activÃ©** dans Docker Desktop :  
  - Ouvrir `Settings > Kubernetes > Enable Kubernetes`.  
  - Choisir le moteur **Kubeadm** (et non Minikube).  

ğŸ’¡ Cela crÃ©e automatiquement un **cluster Kubernetes local** gÃ©rÃ© par Docker Desktop.

---

## 2. CrÃ©ation du Dossier de Projet

CrÃ©er un dossier de travail dans **Visual Studio Code** :

``mkdir Project_DistributedSystems``  
``cd Project_DistributedSystems``

Câ€™est ici que tout le code et les manifests seront placÃ©s :

- ``manifests/`` â†’ pour les fichiers YAML (Deployments, Services, Ingress)
- ``demo-web/`` â†’ pour le code source de la mini application (HTML/CSS)
- ``Dockerfile`` â†’ pour construire lâ€™image

---

## 3. VÃ©rification du Cluster

Une fois Kubernetes activÃ© :

``kubectl get nodes``  
``kubectl get pods -A``

âœ… Vous devriez voir un nÅ“ud **docker-desktop** dans lâ€™Ã©tat *Ready*.

---

## 4. CrÃ©ation des Namespaces (environnements Dev / Test / Prod)

Nous avons mis en place des namespaces pour sÃ©parer les environnements :

``kubectl create namespace dev``  
``kubectl create namespace test``  
``kubectl create namespace prod``

VÃ©rifiez :

``kubectl get ns``

---

## 5. Installation de lâ€™Ingress Controller (NGINX)

Lâ€™**Ingress Controller** permet dâ€™exposer les services Kubernetes vers lâ€™extÃ©rieur du cluster.  
Nous avons choisi **NGINX Ingress Controller**, que lâ€™on installe via la commande suivante :

``kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml``

VÃ©rification :

``kubectl get pods -n ingress-nginx``

Attendez que le pod **ingress-nginx-controller** soit en *Running*.

---

## 6. VÃ©rification de lâ€™installation

Commandes utiles pour vÃ©rifier que tout est bien installÃ© :

``kubectl get all -n ingress-nginx``  
et  
``kubectl get pods -A | findstr ingress``

---

## 7. Configuration du Fichier Hosts (accÃ¨s local au site)

Pour accÃ©der Ã  lâ€™application via un **nom de domaine local**, nous avons ajoutÃ© une ligne dans le fichier hosts :

ğŸ—‚ï¸ **Chemin (Windows)** :
``C:\Windows\System32\drivers\etc\hosts``

Ajouter :
``127.0.0.1   demo.local``

âš ï¸ Cela permet dâ€™accÃ©der Ã  lâ€™application via lâ€™URL :  
ğŸ‘‰ **http://demo.local**

---

## 8. VÃ©rification du Fonctionnement

Quand tout est configurÃ© :

1. Lancer **Docker Desktop** (il dÃ©marre automatiquement Kubernetes).  
2. VÃ©rifier les pods :
   ``kubectl get pods -n dev``  
3. Ouvrir votre navigateur :
   ``http://demo.local``

Vous devriez voir votre page HTML affichÃ©e depuis un pod du cluster Kubernetes.

---

## 9. ArrÃªt propre du systÃ¨me

Ã€ la fin de la journÃ©e :

- Fermer **Docker Desktop**  
  ğŸ‘‰ Cela arrÃªte Kubernetes et libÃ¨re la mÃ©moire.  
  ğŸ‘‰ Aucun fichier ou configuration nâ€™est perdu.

Au redÃ©marrage :

- Rouvrez **Docker Desktop** â†’ tout revient automatiquement (pods, dÃ©ploiements, ingress, etc.).




# ğŸ“š Documentation Projet Kubernetes - Distributed Systems

## ğŸ“‹ Table des MatiÃ¨res
1. [Vue d'ensemble](#vue-densemble)
2. [Configuration du Cluster](#1-configuration-du-cluster)
3. [Pipeline CI/CD](#2-pipeline-cicd)
4. [Configuration de la Base de DonnÃ©es](#3-configuration-de-la-base-de-donnÃ©es)
5. [Monitoring et Scaling](#4-monitoring-et-scaling)
6. [Guide d'Onboarding](#5-guide-donboarding)

---

## ğŸ¯ Vue d'ensemble

**Distributed Systems Demo** est une infrastructure Kubernetes professionnelle dÃ©montrant les concepts avancÃ©s de microservices, sharding MongoDB, cache Redis et pipeline CI/CD automatisÃ©.

### ğŸŒ URLs d'AccÃ¨s
- **Environnement DEV** : `http://demo.local`
- **Environnement TEST** : `http://test.demo.local`
- **Dashboard Kubernetes** : `http://localhost:8001` (aprÃ¨s `kubectl proxy`)

### ğŸ—ï¸ Architecture Technique
Namespace DEV :
- ğŸ”§ MongoDB Sharding Cluster (8 pods)
  - Config Servers (3 pods - replica set rs-config)
  - Shard Servers (3 pods - replica set rs-shard)
  - Mongos Routers (2 pods)
- ğŸ¯ Redis Cluster (2 pods rÃ©pliquÃ©s)
- ğŸš€ Application Flask (3 pods)

Namespace TEST :
- ğŸ—„ï¸ MongoDB RÃ©plication (3 pods - replica set rs0)
- ğŸ¯ Redis (2 pods rÃ©pliquÃ©s)
- ğŸš€ Application Flask (2 pods)
- âš™ï¸ CronJob Auto-Update (exÃ©cution toutes les 5 minutes)

### ğŸ¯ Stack Technologique
- **Application** : Flask (Python) + HTML intÃ©grÃ©
- **Base de donnÃ©es** : MongoDB (sharding/rÃ©plication) + Redis (cache)
- **Orchestration** : Kubernetes + Docker Desktop
- **CI/CD** : GitHub Actions + Docker Hub
- **Monitoring** : Kubernetes Dashboard + Metrics Server

---

## 1. ğŸ“¦ Configuration du Cluster

### 1.1 PrÃ©requis
- âœ… **Docker Desktop** avec Kubernetes activÃ©
- âœ… **kubectl** configurÃ©
- âœ… **Git** pour cloner le repository
- âœ… **AccÃ¨s au fichier hosts** pour la configuration DNS locale

### 1.2 Installation - Environnement Local

#### Ã‰tape 1 : Installation de Docker Desktop
```
# TÃ©lÃ©charger Docker Desktop
# https://www.docker.com/products/docker-desktop/

# Activer Kubernetes
# Docker Desktop â†’ Settings â†’ Kubernetes â†’ Enable Kubernetes

# VÃ©rifier l'installation
kubectl cluster-info
kubectl get nodes
```

#### Ã‰tape 2 : Configuration DNS Locale
Ã‰diter le fichier hosts :
- **Windows** : `C:\Windows\System32\drivers\etc\hosts`
- **Linux/Mac** : `/etc/hosts`

Ajouter les lignes suivantes :
```
127.0.0.1 demo.local
127.0.0.1 test.demo.local
```

#### Ã‰tape 3 : Installation du Cluster (AutomatisÃ©e)
```
git clone <repository-url>
cd Project_DistributedSystems
./start-cluster.sh
```

#### Ã‰tape 4 : Installation Manuelle (Alternative)
```
kubectl apply -f manifests/namespaces.yaml
kubectl apply -f manifests/mongodb-config.yaml -n dev
kubectl apply -f manifests/mongodb-shard.yaml -n dev
kubectl apply -f manifests/mongodb-mongos.yaml -n dev
kubectl apply -f manifests/mongodb-test.yaml -n test
kubectl apply -f manifests/redis-dev.yaml -n dev
kubectl apply -f manifests/redis-test.yaml -n test
kubectl apply -f manifests/web-deployment.yaml -n dev
kubectl apply -f manifests/test-deployment.yaml -n test
./setup-sharding.sh
```

### 1.3 Installation Ingress Controller
```
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml
kubectl get pods -n ingress-nginx
kubectl get services -n ingress-nginx
```

### 1.4 VÃ©rification de l'Installation
```
kubectl get pods -A
curl -I http://demo.local
curl -I http://test.demo.local
kubectl get services -A
kubectl get ingress -A
```

### 1.5 ProcÃ©dure de RedÃ©marrage
```
./start-cluster.sh
./setup-sharding.sh
curl http://demo.local
curl http://test.demo.local
```

### 1.6 ArrÃªt Propre du Cluster
```
# Docker Desktop â†’ ParamÃ¨tres â†’ Kubernetes â†’ Stop
# Attendre l'arrÃªt complet
# Quitter Docker Desktop
```

---

## 2. ğŸ”„ Pipeline CI/CD

### 2.1 Vue d'ensemble du Pipeline
Git Push â†’ Tests Unitaires â†’ Build Docker â†’ Push Docker Hub â†’ Auto-Deploy TEST

### 2.2 Configuration GitHub Actions
```yaml
name: CI/CD Pipeline
on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
jobs:
  build-test-deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
        env:
          PYTHONPATH: .
      - name: Run unit tests
        run: pytest -v
      - name: Build Docker image
        run: |
          docker build -t ${{ secrets.DOCKER_HUB_USERNAME }}/demo-app:latest .
          docker build -t ${{ secrets.DOCKER_HUB_USERNAME }}/demo-app:${{ github.sha }} .
      - name: Push to Docker Hub
        run: |
          echo "${{ secrets.DOCKER_HUB_ACCESS_TOKEN }}" | docker login -u "${{ secrets.DOCKER_HUB_USERNAME }}" --password-stdin
          docker push ${{ secrets.DOCKER_HUB_USERNAME }}/demo-app:latest
          docker push ${{ secrets.DOCKER_HUB_USERNAME }}/demo-app:${{ github.sha }}
```

### 2.3 Secrets GitHub Requis
- `DOCKER_HUB_USERNAME`
- `DOCKER_HUB_ACCESS_TOKEN`

### 2.4 SystÃ¨me d'Auto-Update TEST
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: auto-sync-demo-app
  namespace: test
spec:
  schedule: "*/5 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: auto-sync-sa
          containers:
          - name: auto-sync
            image: alpine/k8s:1.30.8
            command:
            - /bin/sh
            - -c
            - |
              bash /scripts/check-and-update.sh
          volumeMounts:
          - name: script
            mountPath: /scripts
          volumes:
          - name: script
            configMap:
              name: auto-sync-script
```

### 2.5 Tests Unitaires
```python
import pytest
from app.app import app

@pytest.fixture
def client():
    app.testing = True
    with app.test_client() as client:
        yield client

def test_homepage(client):
    response = client.get('/')
    assert response.status_code == 200
```

### 2.6 DÃ©ploiement Zero Downtime
```yaml
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
```

---

## 3. ğŸ—„ï¸ Configuration de la Base de DonnÃ©es

### 3.1 Architecture MongoDB
- DEV : Sharding avancÃ© (Config Servers 3 pods, Shards 3 pods, Mongos 2 pods)
- TEST : RÃ©plication simple (MongoDB 3 pods)

### 3.2 Configuration du Sharding
```bash
./setup-sharding.sh
```

### 3.3 VÃ©rification du Sharding
```bash
kubectl exec -n dev deployment/mongo-mongos -- mongosh --eval "sh.status()"
kubectl exec -n dev deployment/mongo-mongos -- mongosh demoDB --eval "db.hosts.getShardDistribution()"
kubectl exec -n dev mongo-config-0 -- mongosh --eval "rs.status()"
kubectl exec -n dev mongo-shard-0 -- mongosh --eval "rs.status()"
kubectl exec -n test mongo-0 -- mongosh --eval "rs.status()"
```

### 3.4 Configuration Redis
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  namespace: dev
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis:7.2-alpine
        ports:
        - containerPort: 6379
```

### 3.5 StratÃ©gie de Migration
```bash
./refresh-test-db.sh
curl -X POST http://demo.local/api/run-migration
```

### 3.6 Persistent Volumes
```bash
kubectl get pvc -A
kubectl get pv
```

---

## 4. ğŸ“Š Monitoring et Scaling

### 4.1 Kubernetes Dashboard
```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml
kubectl apply -f manifests/dashboard-admin.yaml
kubectl proxy
```

### 4.2 Metrics Server
```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
kubectl -n kube-system patch deployment metrics-server --type='json' -p='[{"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kubelet-insecure-tls"}]'
kubectl top pods -A
kubectl top nodes
```

### 4.3 Commandes de Monitoring
```bash
kubectl get pods -A -w
kubectl top pods -A
kubectl logs -n dev -l app=demo-app --tail=20
kubectl logs -f deployment/demo-app -n dev
kubectl get events -A --sort-by='.lastTimestamp'
kubectl get deployments -A
```

### 4.4 Scaling Manuel
```bash
kubectl scale deployment/demo-app --replicas=5 -n dev
kubectl scale deployment/demo-app --replicas=3 -n test
kubectl get pods -n dev
kubectl get pods -n test
kubectl rollout status deployment/demo-app -n dev
```
Ou bien juste allez sur le dashboard Kubernetes et mettre Ã  l'Ã©chelles les rÃ©plicas qu'on souhaite scaller.

---

## 5. ğŸ‘¨â€ğŸ’» Guide d'Onboarding

### 5.1 Installation Express (10 minutes)
```bash
git clone <repository-url>
cd Project_DistributedSystems
# Configurer hosts
./start-cluster.sh
kubectl get all -n dev
kubectl get all -n test
```

### 5.2 Environnements Disponibles
| Environnement | URL | Usage | Architecture DB | Auto-Update |
|---------------|-----|-------|-----------------|-------------|
| DEV           | demo.local | DÃ©veloppement | Sharding MongoDB | Non |
| TEST          | test.demo.local | Tests | RÃ©plication MongoDB | Oui (5 min) |

### 5.3 Workflow de DÃ©veloppement
```bash
git checkout -b feature/ma-nouvelle-feature
# DÃ©velopper, tester
pytest -v
docker build -t demo-app:test .
docker run -p 5000:5000 demo-app:test
git add .
git commit -m "feat: description de la fonctionnalitÃ©"
git push origin feature/ma-nouvelle-feature
```

### 5.4 Commandes de DÃ©veloppement Quotidiennes
```bash
docker build -t <username>/demo-app:v2 .
docker run -p 5000:5000 <username>/demo-app:v2
kubectl set image deployment/demo-app demo-app=<username>/demo-app:v2 -n dev
kubectl rollout restart deployment/demo-app -n dev
```

### 5.5 Endpoints API Disponibles
- GET /  
- GET /user-dashboard  
- GET /api/stats  
- GET /api/users  
- GET /api/orders  
- POST /api/load-sample-data  
- POST /api/random-user  
- POST /api/random-order  
- POST /api/run-migration  
- DELETE /api/clear-data  
- GET /cache/status  
- POST /cache/clear  
- GET /sharding-info  

### 5.6 RÃ©solution de ProblÃ¨mes Courants
- Application inaccessible : vÃ©rifier pods, services, ingress, logs, restart
- MongoDB inaccessible : reconfigurer sharding, logs, test ping
- DonnÃ©es corrompues : refresh TEST depuis DEV, reload sample, clear data
- Cache Redis non fonctionnel : vÃ©rifier, clear, status

### 5.7 DÃ©pannage Rapide
```bash
./start-cluster.sh
./setup-sharding.sh
./refresh-test-db.sh
kubectl rollout restart deployment/demo-app -n dev
kubectl rollout restart deployment/demo-app -n test
```

### 5.8 Conventions de DÃ©veloppement
- Tests unitaires obligatoires
- Build Docker fonctionnel requis
- Messages de commit clairs
- VÃ©rifier logs aprÃ¨s chaque dÃ©ploiement
- Utiliser endpoints existants

"""

