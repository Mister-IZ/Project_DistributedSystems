#!/bin/bash
echo "ğŸš€ CONFIGURATION SHARDING MONGODB "

# Attendre que les services soient prÃªts
echo "â³ Attente initiale pour la stabilitÃ© des services..."
sleep 30

# 1. VÃ‰RIFICATION ET INITIALISATION DES CONFIG SERVERS
echo "ğŸ” VÃ©rification des config servers..."
kubectl exec -n dev mongo-config-0 -- mongosh --eval "
try {
    var status = rs.status();
    print('âœ… Config servers dÃ©jÃ  initialisÃ©s - Status: ' + status.ok);
} catch (e) {
    if (e.codeName === 'NotYetInitialized') {
        print('âš™ï¸ Initialisation des config servers...');
        rs.initiate({
            _id: 'rs-config',
            configsvr: true,
            members: [
                { _id: 0, host: 'mongo-config-0.mongo-config.dev.svc.cluster.local:27017' },
                { _id: 1, host: 'mongo-config-1.mongo-config.dev.svc.cluster.local:27017' },
                { _id: 2, host: 'mongo-config-2.mongo-config.dev.svc.cluster.local:27017' }
            ]
        });
        print('âœ… Config servers initialisÃ©s avec succÃ¨s');
    } else {
        print('âŒ Erreur config servers: ' + e.message);
    }
}
" || echo "âš ï¸ ProblÃ¨me avec config servers"

# 2. VÃ‰RIFICATION ET INITIALISATION DU SHARD
echo "ğŸ”§ VÃ©rification du shard replica set..."
kubectl exec -n dev mongo-shard-0 -- mongosh --eval "
try {
    var status = rs.status();
    print('âœ… Shard dÃ©jÃ  initialisÃ© - Status: ' + status.ok);
} catch (e) {
    if (e.codeName === 'NotYetInitialized') {
        print('âš™ï¸ Initialisation du shard...');
        rs.initiate({
            _id: 'rs-shard',
            members: [
                { _id: 0, host: 'mongo-shard-0.mongo-shard.dev.svc.cluster.local:27017' },
                { _id: 1, host: 'mongo-shard-1.mongo-shard.dev.svc.cluster.local:27017' },
                { _id: 2, host: 'mongo-shard-2.mongo-shard.dev.svc.cluster.local:27017' }
            ]
        });
        print('âœ… Shard initialisÃ© avec succÃ¨s');
    } else {
        print('âŒ Erreur shard: ' + e.message);
    }
}
" || echo "âš ï¸ ProblÃ¨me avec shard"

# 3. ATTENTE STABILISATION
echo "â³ Attente de stabilisation des replica sets (30 secondes)..."
sleep 30

# 4. VÃ‰RIFICATION QUE MONGOS EST OPÃ‰RATIONNEL
echo "ğŸ¯ VÃ©rification de mongos..."
kubectl exec -n dev deployment/mongo-mongos -- mongosh --eval "db.adminCommand('ping')" > /dev/null 2>&1

if [ $? -ne 0 ]; then
    echo "ğŸ”„ RedÃ©marrage de mongos (non opÃ©rationnel)..."
    kubectl rollout restart deployment/mongo-mongos -n dev
    echo "â³ Attente du redÃ©marrage de mongos (25 secondes)..."
    sleep 25
fi

# 5. CONFIGURATION DU SHARDING VIA MONGOS
echo "âš™ï¸ Configuration du sharding via mongos..."

kubectl exec -n dev deployment/mongo-mongos -- mongosh --eval "
print('ğŸ¯ DÃ©but configuration sharding...');

// Test de connexion aux config servers
try {
    var shards = db.adminCommand({ listShards: 1 });
    print('âœ… ConnectÃ© aux config servers - ' + shards.shards.length + ' shard(s) listÃ©(s)');
} catch (e) {
    print('âŒ Impossible d\\'accÃ©der aux config servers: ' + e.message);
    quit(1);
}

// 1. Ajouter le shard
try {
    sh.addShard('rs-shard/mongo-shard-0.mongo-shard.dev.svc.cluster.local:27017');
    print('âœ… Shard ajoutÃ©');
} catch(e) { 
    print('â„¹ï¸ Shard: ' + e.message); 
}

// 2. Activer sharding sur demoDB
try {
    sh.enableSharding('demoDB');
    print('âœ… Sharding activÃ© sur demoDB');
} catch(e) { 
    print('â„¹ï¸ Sharding: ' + e.message); 
}

// 3. PrÃ©parer la base
print('ğŸ—‚ï¸ PrÃ©paration des collections...');
db = db.getSiblingDB('demoDB');

try { 
    db.dropDatabase();
    print('âœ… Base demoDB rÃ©initialisÃ©e');
} catch(e) { 
    print('â„¹ï¸ Base: ' + e.message); 
}

// RecrÃ©er les collections
db.createCollection('users');
db.createCollection('orders');
db.createCollection('hosts');
print('âœ… Collections crÃ©Ã©es');

// 4. Sharder users
try {
    db.users.createIndex({ user_id: 'hashed' });
    sh.shardCollection('demoDB.users', { user_id: 'hashed' });
    print('âœ… Users shardÃ© sur user_id');
} catch(e) { 
    print('âŒ Users: ' + e.message); 
}

// 5. Sharder orders
try {
    db.orders.createIndex({ order_id: 'hashed' });
    sh.shardCollection('demoDB.orders', { order_id: 'hashed' });
    print('âœ… Orders shardÃ© sur order_id');
} catch(e) { 
    print('âŒ Orders: ' + e.message); 
}

// 6. Sharder hosts
try {
    db.hosts.createIndex({ _id: 'hashed' });
    sh.shardCollection('demoDB.hosts', { _id: 'hashed' });
    print('âœ… Hosts shardÃ© sur _id');
} catch(e) { 
    print('âŒ Hosts: ' + e.message); 
}

print('ğŸ” VÃ©rification finale...');
"

# 6. VÃ‰RIFICATION DÃ‰TAILLÃ‰E
echo "ğŸ” VÃ©rification dÃ©taillÃ©e..."
kubectl exec -n dev deployment/mongo-mongos -- mongosh --eval "
print('=== SHARDING STATUS ===');
sh.status();

print('\\\\n=== DÃ‰TAILS demoDB ===');
var dbInfo = db.getSiblingDB('config').databases.findOne({_id: 'demoDB'});
if (dbInfo) {
    print('demoDB partitioned: ' + dbInfo.partitioned);
    print('demoDB primary: ' + dbInfo.primary);
} else {
    print('âŒ demoDB non trouvÃ©e dans config');
}

print('\\\\n=== COLLECTIONS SHARDÃ‰ES ===');
db = db.getSiblingDB('demoDB');
var collections = db.getCollectionNames();
print('Collections: ' + JSON.stringify(collections));

collections.forEach(function(coll) {
    try {
        var stats = db[coll].stats();
        print('- ' + coll + ': sharded=' + stats.sharded + ', count=' + stats.count);
    } catch(e) {
        print('- ' + coll + ': erreur stats');
    }
});
"

# 7. TEST FINAL CRITIQUE
echo "ğŸ¯ Test final critique..."
kubectl exec -n dev deployment/mongo-mongos -- mongosh demoDB --eval "
try {
    var result = db.users.getShardDistribution();
    print('âœ…âœ…âœ… SUCCÃˆS! Users shardÃ©:');
    print(JSON.stringify(result, null, 2));
} catch(e) {
    print('âŒâŒâŒ Ã‰CHEC - Users pas shardÃ©: ' + e.message);
}

try {
    var result = db.orders.getShardDistribution();
    print('âœ…âœ…âœ… SUCCÃˆS! Orders shardÃ©:');
    print(JSON.stringify(result, null, 2));
} catch(e) {
    print('âŒâŒâŒ Ã‰CHEC - Orders pas shardÃ©: ' + e.message);
}
"

echo ""
echo "ğŸ‰ CONFIGURATION SHARDING TERMINÃ‰E !"